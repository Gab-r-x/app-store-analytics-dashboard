import requests
import time
import random
import unicodedata
from bs4 import BeautifulSoup
import logging
from config import settings
from bson import ObjectId  # Import para tratar ObjectId

USER_AGENTS = settings.USER_AGENTS
MAX_RETRIES = settings.MAX_RETRIES
TIME_BETWEEN_REQUESTS = settings.TIME_BETWEEN_REQUESTS

# Logging configuration
logging.basicConfig(
    filename="logs/scraper_app_details.log",
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)

def clean_text(text):
    """Remove caracteres especiais e normaliza encoding."""
    if text:
        return unicodedata.normalize("NFKC", text).replace("\xa0", " ").strip()
    return None

def get_headers():
    """Returns a random User-Agent header to avoid blocking."""
    return {"User-Agent": random.choice(USER_AGENTS)}

def random_delay():
    """Applies a random delay between requests to prevent IP bans."""
    delay = random.uniform(*TIME_BETWEEN_REQUESTS)
    logging.info(f"Waiting {delay:.2f} seconds before the next request.")
    time.sleep(delay)

def get_page_with_retry(url):
    """Attempts to retrieve a webpage with automatic retries for 429 errors."""
    attempt = 0
    while attempt < MAX_RETRIES:
        response = requests.get(url, headers=get_headers())

        if response.status_code == 200:
            return response
        
        elif response.status_code == 429:
            wait_time = 2 ** attempt  # Exponential backoff (2s, 4s, 8s...)
            logging.warning(f"âš ï¸ Rate limited (429) at {url}. Retrying in {wait_time} seconds...")
            time.sleep(wait_time)

        else:
            logging.error(f"âŒ Error accessing {url}: {response.status_code}")
            return None
        
        attempt += 1
    
    logging.error(f"ðŸš¨ Failed after {MAX_RETRIES} attempts. Skipping {url}.")
    return None

def get_app_details(app_url):
    """Scrapes app details from the App Store."""
    logging.info(f'ðŸ” Scraping: {app_url}')
    response = get_page_with_retry(app_url)
    if not response:
        return None

    soup = BeautifulSoup(response.text, 'html.parser')
    details = {}

    # TÃ­tulo
    title = soup.find("h1", class_="product-header__title")
    details["Title"] = clean_text(title.text) if title else None
    
    # SubtÃ­tulo
    subtitle = soup.find("h2", class_="product-header__subtitle")
    details["Subtitle"] = clean_text(subtitle.text) if subtitle else None

    # Desenvolvedor
    developer = soup.find("h2", class_="product-header__identity")
    details["Developer"] = clean_text(developer.text) if developer else None
    
    # App Url
    details["Url"] = app_url

    # Rank da categoria
    category_rank = soup.select_one(".product-header__list__item a")
    details["Category Rank"] = clean_text(category_rank.text) if category_rank else None

    # AvaliaÃ§Ã£o
    rating_info = soup.select_one(".we-rating-count.star-rating__count")
    details["Rating"] = clean_text(rating_info.text) if rating_info else None

    # PreÃ§o
    price_info = soup.select_one(".inline-list__item--bulleted")
    details["Price"] = clean_text(price_info.text) if price_info else None

    # Capturas de tela
    screenshots = []
    for picture in soup.select("picture.we-artwork"):
        sources = picture.find_all("source")
        best_quality = None
        for source in sources:
            srcset = source.get("srcset")
            if srcset:
                best_quality = srcset.split(",")[-1].strip().split(" ")[0]
        if best_quality:
            screenshots.append(best_quality)
    details["Screenshots"] = screenshots    

    # DescriÃ§Ã£o
    description = soup.select_one(".section__description p")
    details["Description"] = clean_text(description.text) if description else None

    # Ãšltima versÃ£o
    latest_version = soup.select_one(".whats-new__latest__version")
    latest_version_date = soup.select_one(".whats-new__latest time")
      
    details["Latest Version"] = clean_text(latest_version.text) if latest_version else None
    details["Latest Version Date"] = clean_text(latest_version_date.text) if latest_version_date else None

    # AvaliaÃ§Ãµes
    reviews = []
    reviews_page_url = app_url + "?see-all=reviews"
    response_reviews = requests.get(reviews_page_url, headers=get_headers())
    if response_reviews.status_code == 200:
        soup_reviews = BeautifulSoup(response_reviews.text, 'html.parser')
        for review in soup_reviews.select(".we-customer-review"):
            reviews.append({
                "rating": clean_text(review.select_one(".we-star-rating").get("aria-label")) if review.select_one(".we-star-rating") else None,
                "author": clean_text(review.select_one(".we-customer-review__user").text) if review.select_one(".we-customer-review__user") else None,
                "date": clean_text(review.select_one(".we-customer-review__date").text) if review.select_one(".we-customer-review__date") else None,
                "title": clean_text(review.select_one(".we-customer-review__title").text) if review.select_one(".we-customer-review__title") else None,
                "body": clean_text(review.select_one(".we-customer-review__body p").text) if review.select_one(".we-customer-review__body p") else None
            })
    details["Reviews"] = reviews

    # Dados de privacidade
    privacy = []
    for item in soup.select(".privacy-type__data-category-heading"):
        privacy.append(clean_text(item.text))
    details["Privacy Data"] = privacy

    # InformaÃ§Ãµes gerais
    general_info = {}
    for info in soup.select(".information-list__item"):
        key = info.find("dt")
        value = info.find("dd")
        if key and value:
            general_info[clean_text(key.text)] = clean_text(value.text)
    details["General Info"] = general_info

    logging.info(f"âœ… Data collected for app: {details.get('Title', 'Unknown')}")    

    random_delay()  # Adds delay to prevent blocking

    return details
